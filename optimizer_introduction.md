# 深度学习优化器介绍与超参数敏感性分析

本文档详细介绍了本项目中使用的几种主流优化器（SGD, RMSprop, Adagrad, Adam, AdamW）的原理特点、适用场景以及对关键超参数的敏感性。

## 1. SGD (Stochastic Gradient Descent)

### 1.1 基本原理
随机梯度下降（SGD）是最基础的优化算法。在每次更新时，它从训练集中随机抽取一个样本（或一个小批量 batch）来计算梯度并更新参数。
- **无动量 (No Momentum)**: 直接沿梯度的反方向更新参数。容易陷入局部极小值或鞍点，且在沟壑状的损失曲面上震荡严重。
- **带动量 (Momentum)**: 引入“动量”概念，模拟物理中的惯性。当前的更新方向不仅取决于当前梯度，还保留了历史更新方向的一部分。这有助于加速收敛并减少震荡。

### 1.2 特点
- **优点**: 泛化能力通常较好（收敛到的极小值往往具有较好的平坦性）。
- **缺点**: 收敛速度相对较慢，容易陷入鞍点（无动量时），对学习率非常敏感。

### 1.3 超参数敏感性
- **`lr` (学习率)**: **极度敏感**。
  - 太大：导致震荡甚至发散。
  - 太小：收敛极慢，容易停留在局部最优。
- **`momentum` (动量因子)**: **敏感**。
  - 常用值：0.9。
  - 作用：值越大，历史梯度的影响越大，加速效果越明显，但过大可能导致在极值点附近“冲过头”。
- **`weight_decay`**: 对 CNN 等模型的泛化性能影响较大。

---

## 2. Adagrad (Adaptive Gradient Algorithm)

### 2.1 基本原理
Adagrad 是一种自适应学习率算法。它为每个参数独立调整学习率：对于频繁更新（梯度大）的参数，减小其学习率；对于稀疏更新（梯度小）的参数，增大其学习率。实现方式是累积历史梯度的平方和，用其平方根作为分母来缩放学习率。

### 2.2 特点
- **优点**: 特别适合处理稀疏数据（如 NLP 中的词向量）。无需手动调整学习率调度（Learning Rate Schedule）。
- **缺点**: 分母中累积的梯度平方和会单调递增，导致学习率单调递减。在训练后期，学习率可能变得无限小，导致模型提前停止学习（"早停"现象）。

### 2.3 超参数敏感性
- **`lr` (初始学习率)**: **敏感**。
  - 因为学习率只会减小，初始值必须足够大（通常比 SGD/Adam 大，如 0.01），否则很快就会无法更新。
- **`eps`**: 用于防止除零的小常数，通常影响不大。

---

## 3. RMSprop (Root Mean Square Propagation)

### 3.1 基本原理
RMSprop 是为了解决 Adagrad 学习率急剧下降问题而提出的。它修改了梯度平方和的累积方式，使用**指数加权移动平均**（Exponential Moving Average）来计算历史梯度平方。这意味着它只关注“最近”的梯度大小，不再让分母无限累积。

### 3.2 特点
- **优点**: 解决了 Adagrad 训练后期学习率过小的问题，适合处理非平稳目标（如 RNN）。
- **缺点**: 依然需要调整全局学习率。

### 3.3 超参数敏感性
- **`lr`**: **敏感**。通常设为 0.001 或 0.0001。
- **`alpha` (平滑常数)**: 控制历史梯度的衰减速率。
  - 常用值：0.99。
  - 含义：值越小，历史信息遗忘越快，适应性越强但越不稳定。

---

## 4. Adam (Adaptive Moment Estimation)

### 4.1 基本原理
Adam 结合了 Momentum 和 RMSprop 的优点。它同时计算：
1. **一阶矩估计**（梯度的指数移动平均，类似 Momentum）。
2. **二阶矩估计**（梯度平方的指数移动平均，类似 RMSprop）。
利用这两个矩来动态调整每个参数的学习率。

### 4.2 特点
- **优点**: 收敛速度快，对超参数相对不敏感（默认参数通常效果不错），占用内存稍多（需要存储两个状态量）。是目前深度学习中最常用的优化器之一。
- **缺点**: 在某些任务上（尤其是图像分类），其泛化性能可能略逊于精调的 SGD。

### 4.3 超参数敏感性
- **`lr`**: **最敏感**。通常在 1e-3 到 1e-5 之间。
- **`betas` (β1, β2)**:
  - `β1` (一阶矩衰减，默认 0.9)：控制动量。
  - `β2` (二阶矩衰减，默认 0.999)：控制自适应步长。如果梯度非常稀疏或不稳定，可能需要调整。
- **`weight_decay`**: 在标准 Adam 中，L2 正则化和权重衰减并不完全等价，这导致了 AdamW 的出现。

---

## 5. AdamW (Adam with Decoupled Weight Decay)

### 5.1 基本原理
AdamW 是 Adam 的一个变体，它修正了 Adam 中权重衰减（Weight Decay）的实现方式。在标准 Adam 中，L2 正则化是加在梯度上的；而在 AdamW 中，权重衰减直接作用于参数更新步骤（与梯度更新解耦）。

### 5.2 特点
- **优点**: 理论上更正确。在许多任务（特别是 Transformer 和 ResNet 等现代架构）中，AdamW 通常比 Adam 具有更好的泛化性能和收敛稳定性。
- **缺点**: 需要单独调整权重衰减系数。

### 5.3 超参数敏感性
- **`lr`**: **敏感**。与 Adam 类似。
- **`weight_decay`**: **敏感**。
  - AdamW 的核心优势在于正确的权重衰减，因此这个参数对最终模型的泛化能力至关重要。
  - 典型范围：1e-4 到 0.05。

---

## 总结对比表

| 优化器 | 核心机制 | 适用场景 | 优点 | 缺点 | 关键超参数 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **SGD** | 梯度下降 + 动量 | 计算机视觉 (CV) | 泛化性好，内存占用低 | 收敛慢，难调参 | `lr`, `momentum` |
| **Adagrad** | 累积梯度平方 | 稀疏数据 (NLP) | 自动调整学习率 | 学习率下降过快 | `lr` (需设大) |
| **RMSprop** | 移动平均梯度平方 | RNN, 强化学习 | 解决 Adagrad 早停问题 | 仍需调 lr | `lr`, `alpha` |
| **Adam** | 动量 + 自适应步长 | 通用，快速原型 | 收敛快，易调参 | 泛化性有时不如 SGD | `lr` |
| **AdamW** | Adam + 解耦权重衰减 | Transformer, 现代 CNN | 泛化性优于 Adam | 需调权重衰减 | `lr`, `weight_decay` |
