# ResNet-18 模型实验报告

## 1. ResNet-18 模型简介

ResNet (Residual Network) 是由微软研究院在 2015 年提出的深度卷积神经网络，它通过引入"残差学习"（Residual Learning）解决了深层网络难以训练的问题（梯度消失/爆炸）。

### 1.1 核心思想：残差块 (Residual Block)
传统的卷积网络试图直接学习目标映射 $H(x)$，而 ResNet 试图学习残差映射 $F(x) = H(x) - x$。
即：$H(x) = F(x) + x$。
其中 $x$ 是输入，Identity Mapping（恒等映射）直接将输入传到输出。这使得网络更容易学习恒等映射，从而允许训练更深的网络。

### 1.2 ResNet-18 结构
ResNet-18 包含 18 层深度的网络结构：
*   **输入层**：7x7 卷积
*   **中间层**：4 个残差层（Layer 1-4），每个层包含 2 个残差块（BasicBlock），每个块包含 2 个 3x3 卷积。
*   **输出层**：全局平均池化 + 全连接层

---

## 2. 针对 Fashion-MNIST 的适配

原始 ResNet-18 是为 ImageNet (224x224 RGB图像) 设计的。为了适应 Fashion-MNIST (28x28 单通道灰度图)，我们进行了以下关键修改：

### 2.1 输入层修改
*   **原版**：`nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)`
*   **修改版**：`nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)`
*   **原因**：
    1.  **通道数**：输入从 3 (RGB) 改为 1 (灰度)。
    2.  **卷积核与步长**：原版 7x7 卷积配合 stride=2 会让图片尺寸瞬间减半（28->14），对于本就很小的 Fashion-MNIST 图片，这会丢失太多细节。改为 3x3 stride=1 可以保持原始分辨率。

### 2.2 移除池化层
*   **操作**：移除了第一层卷积后的 `MaxPool2d`。
*   **原因**：原版 MaxPool 会再次将尺寸减半。如果保留，输入 28x28 的图片经过第一层和池化后会变成 7x7，经过后续层后特征图会变得过小（如 1x1），无法进行有效的特征提取。

### 2.3 输出层修改
*   **操作**：将全连接层的输出维度从 1000 (ImageNet 类别) 改为 10 (Fashion-MNIST 类别)。

---

## 3. 改进版 ResNet-18 (ResNet18_Improved)

为了进一步提升性能并防止过拟合（Overfitting），我们在基础版之上实施了以下改进策略：

### 3.1 数据增强 (Data Augmentation)
在训练数据加载器中增加了随机变换，增加数据多样性，提高模型泛化能力。
*   `RandomHorizontalFlip()`: 随机水平翻转。
*   `RandomCrop(28, padding=4)`: 随机填充后裁剪，模拟物体位置的微小变化。
*   `RandomRotation(10)`: 随机旋转 ±10 度。

### 3.2 正则化策略
*   **Dropout**：在全连接层前增加了 `Dropout(p=0.5)`，随机丢弃 50% 的神经元，防止网络过度依赖某些特定特征。
*   **Weight Decay (L2 正则化)**：在优化器中设置 `weight_decay=1e-4`，惩罚过大的权重值，限制模型复杂度。

### 3.3 学习率衰减 (Learning Rate Decay)
*   **策略**：使用 `StepLR` 调度器。
*   **设置**：每隔 5 个 Epoch 将学习率乘以 0.1。
*   **目的**：
    *   **前期**：使用较大的学习率（0.001）快速收敛。
    *   **后期**：使用较小的学习率精细调整，帮助模型收敛到损失函数的局部极小值，避免在最优解附近震荡。

## 4. 实验总结

通过上述改进，模型应当表现出以下特征：
1.  **训练初期**：由于数据增强和 Dropout 的存在，训练集准确率可能略低于测试集准确率。
2.  **训练后期**：随着学习率衰减，准确率会有明显的台阶式提升。
3.  **最终结果**：相比未改进版本，改进版在测试集上的准确率更高，且泛化能力更强（训练集和测试集准确率差距更小）。
